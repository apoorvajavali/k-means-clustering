{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Tweets..\n",
      "\n",
      "Clustering done\n",
      "\n",
      "Sum of Squared Error: 1578.5526409467507\n",
      "Cluster 1: 175 tweets\n",
      "Cluster 2: 53 tweets\n",
      "Cluster 3: 49 tweets\n",
      "Cluster 4: 121 tweets\n",
      "Cluster 5: 9 tweets\n",
      "Cluster 6: 83 tweets\n",
      "Cluster 7: 21 tweets\n",
      "Cluster 8: 63 tweets\n",
      "Cluster 9: 151 tweets\n",
      "Cluster 10: 62 tweets\n",
      "Cluster 11: 67 tweets\n",
      "Cluster 12: 13 tweets\n",
      "Cluster 13: 45 tweets\n",
      "Cluster 14: 119 tweets\n",
      "Cluster 15: 220 tweets\n",
      "Cluster 16: 59 tweets\n",
      "Cluster 17: 18 tweets\n",
      "Cluster 18: 176 tweets\n",
      "Cluster 19: 104 tweets\n",
      "Cluster 20: 32 tweets\n",
      "Cluster 21: 178 tweets\n",
      "Cluster 22: 8 tweets\n",
      "Cluster 23: 17 tweets\n",
      "Cluster 24: 108 tweets\n",
      "Cluster 25: 48 tweets\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import decimal\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Preprocess data by removing tweet id and timestamp,\n",
    "# URL, word beginning with @, # (replaces #word with word)\n",
    "# converts to lowercase\n",
    "def preprocess(file):\n",
    "    lines = file.readlines()\n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"|\")\n",
    "        tweet, tweet_id = data[-1].lower(), data[0] \n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        tweet = re.sub(r'#(\\S+)', r'\\1', tweet)\n",
    "        tweet = re.sub(r'@[\\S]+', '', tweet)\n",
    "        tweet = re.sub(r'\\brt\\b', ' ', tweet)\n",
    "        words = re.findall(r\"[\\w]+\", tweet)\n",
    "        if len(words) != 0:\n",
    "            wordset = set(words)\n",
    "            dataset.append({\"id\":tweet_id,\"words\":wordset})\n",
    "    return dataset\n",
    "\n",
    "# Computes Jaccard distance between two sets\n",
    "def computeJaccardDistance(set1, set2):\n",
    "    return 1 - (len(set1.intersection(set2)) / (len(set1.union(set2)))) \n",
    "\n",
    "# Updates each centroid with the tweet having minimum \n",
    "# distance to all other tweets in a cluster\n",
    "def updateCentroids(cluster, tweet_ids, tweets, k, centroids):\n",
    "    indices = []\n",
    "    new_centroid_index = []\n",
    "    new_centroid = []\n",
    "    for i in range(k):\n",
    "        indices.append([j for j, u in enumerate(cluster) if u == i])\n",
    "        m = indices[i]\n",
    "\n",
    "        if (len(m) != 0):\n",
    "            new_tweets = [tweets[p] for p in m]\n",
    "            distances = [[computeJaccardDistance(new_tweets[i], new_tweets[j]) for j in range(len(m))] for i in range(len(m))]\n",
    "            dist_sum = [sum(i) for i in distances]\n",
    "            minIndex = dist_sum.index(min(dist_sum))\n",
    "            new_centroid_index.append(m[minIndex]) \n",
    "        else:\n",
    "            new_centroid_index.append(tweet_ids.index(centroids[i]))\n",
    "    new_centroid = [tweet_ids[x] for x in new_centroid_index]\n",
    "    return new_centroid\n",
    "      \n",
    "# Computes sum of squared error    \n",
    "def computeSSE(cluster, centroids, tweet_ids, tweets, k):\n",
    "    indices_temp = []\n",
    "    indices = [tweet_ids.index(item) for item in centroids]\n",
    "    tweet_words = [tweets[x] for x in indices]\n",
    "    sse = 0\n",
    "    for i in range(k):\n",
    "        indices_temp.append([j for j, u in enumerate(cluster) if u == i])\n",
    "        t = [tweets[x] for x in indices_temp[i]]\n",
    "        for m in range(len(indices_temp[i])):\n",
    "            sse += computeJaccardDistance(t[m], tweet_words[i])**2\n",
    "    return sse\n",
    "\n",
    "# Prints size of each cluster\n",
    "def printClusterSize(cluster, k):\n",
    "    clusters = []\n",
    "    for i in range(k):\n",
    "        clusters.append([c for c in cluster if c == i])\n",
    "    for i in range(k):\n",
    "        print(\"Cluster \" + str(i+1) + \": \" + str(len(clusters[i])) + \" tweets\")\n",
    "\n",
    "# Runs KMeans Clustering \n",
    "def kmeansClustering(k, centroids, tweet_ids, tweets):  \n",
    "    print(\"Clustering Tweets..\")\n",
    "    converged = False  \n",
    "    while not converged:\n",
    "        converged = True\n",
    "        clusterArray = []\n",
    "        indices = [tweet_ids.index(t_id) for t_id in centroids]\n",
    "        centroid_tweets = [tweets[i] for i in indices]\n",
    "\n",
    "        for t in tweets:\n",
    "            distance = [computeJaccardDistance(t, c) for c in centroid_tweets]\n",
    "            minIndex = distance.index(min(distance))\n",
    "            clusterArray.append(minIndex)\n",
    "\n",
    "        new_centroids = updateCentroids(clusterArray, tweet_ids, tweets, k, centroids)\n",
    "        \n",
    "        for i in range(k):\n",
    "            converged = converged and (new_centroids[i] == centroids[i])\n",
    "        if not converged:\n",
    "            centroids[:] = new_centroids[:]\n",
    "    print(\"\\nClustering done\\n\")\n",
    "    sse = computeSSE(clusterArray, centroids, tweet_ids, tweets, k)\n",
    "    print('Sum of Squared Error: ' + str(sse))\n",
    "    printClusterSize(clusterArray, k)\n",
    "    \n",
    "\n",
    "tweetFile = open(\"foxnewshealth.txt\")\n",
    "dataset = preprocess(tweetFile)\n",
    "tweet_ids = [dataset[i][\"id\"] for i in range(len(dataset))]\n",
    "tweets = [dataset[i][\"words\"] for i in range(len(dataset))]\n",
    "initial_centroids = random.sample(tweet_ids, 25)\n",
    "\n",
    "\n",
    "kmeansClustering(25, initial_centroids, tweet_ids, tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
